{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6pt9oF3eAsS"
   },
   "source": [
    "# 7.g Modele encoder-decoder i mechanizm uwagi\n",
    "\n",
    "Zbudujemy model służący to tłumaczenia krótkich zdań w języku polskim na język angielski. Model będzie oparty o architekturę encoder-decoder, wykorzystującą mechanizm uwagi. Warstwy rekurencyjne to warstwy GRU.\n",
    "Dane pochodzą z korpusu Open Subtitles: http://opus.nlpl.eu/OpenSubtitles-v2018.php Dane poddano selekcji i filtrowaniu.\n",
    "\n",
    "Notatnik jest oparty o tutorial do budowania chatbotów którego autorem jest Matthew Inkawhich, umieszczony na stronie pytorcha: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4397,
     "status": "ok",
     "timestamp": 1610288063628,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "PKyR2JeHgeNP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxdNT8ZqNlG5"
   },
   "source": [
    "Pobranie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22496,
     "status": "ok",
     "timestamp": 1610288081737,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "ehfCTlbL2ZPi",
    "outputId": "3bd2b2cb-102a-4c43-ea89-8117fa732258"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/sagespl/nlp-masterclass/blob/main/modu%C5%82-07/encoder.model?raw=true\n",
    "!mv encoder.model?raw=true encoder.model\n",
    "\n",
    "!wget https://github.com/sagespl/nlp-masterclass/blob/main/modu%C5%82-07/decoder.model?raw=true\n",
    "!mv decoder.model?raw=true decoder.model\n",
    "\n",
    "!wget https://github.com/sagespl/nlp-masterclass/blob/main/modu%C5%82-07/seq2seq_data.zip?raw=true\n",
    "!mv seq2seq_data.zip?raw=true seq2seq_data.zip\n",
    "!unzip seq2seq_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25753,
     "status": "ok",
     "timestamp": 1610288084997,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "qAgGr6nn03Lh"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"seq2seq_data/data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"seq2seq_data/pl_voc.json\") as f: # korzystamy z gotowego słownika\n",
    "    pl_voc = json.load(f) # 15000 kluczy\n",
    "\n",
    "with open(\"seq2seq_data/en_voc.json\") as f:\n",
    "    en_voc = json.load(f) # 10000 kluczy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bfZRB2gPFRD"
   },
   "source": [
    "Rzut oka na dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25749,
     "status": "ok",
     "timestamp": 1610288084998,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "Dj3l_QykO_eh",
    "outputId": "2faeb825-dc79-4a07-fcdc-088fe1b23b70"
   },
   "outputs": [],
   "source": [
    "print(pl_voc[:10])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tk1kYbQJjyY"
   },
   "source": [
    "#### Definicja funkcji służacej do przygotowywania danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25747,
     "status": "ok",
     "timestamp": 1610288084998,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "m00mr1mYz0fm"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "MAXLEN = 10 +1 # maksymalna długość sekwencji w danych + token EOS\n",
    "# definiujemy tokeny specjalne\n",
    "PL_PAD_TOKEN = len(pl_voc) + 0\n",
    "PL_EOS_TOKEN = len(pl_voc) + 1\n",
    "\n",
    "EN_PAD_TOKEN = len(en_voc) + 0\n",
    "EN_SOS_TOKEN = len(en_voc) + 1 # token oznaczający początek sekwencji\n",
    "EN_EOS_TOKEN = len(en_voc) + 2 # token oznaczający koniec sekwencji\n",
    "\n",
    "def examples_to_batch(examples, maxlen):\n",
    "    pl_ids, en_ids = [], []\n",
    "    for pl, en in examples:\n",
    "        pl_ids.append([w for w in pl]) # kopiowanie list idków\n",
    "        en_ids.append([w for w in en] + [EN_EOS_TOKEN]) # kopiowanie list idków + dodanie tokenu oznaczającego koniec sekwencji\n",
    "\n",
    "    len_indices = [(i, len(toks)) for i, toks in list(enumerate(pl_ids))] \n",
    "    len_indices = sorted(len_indices, key=lambda x:x[1], reverse=True) \n",
    "    pl_ids = [pl_ids[i[0]] for i in len_indices] # sortowanie inputu i targetu zgodnie z długością inputu\n",
    "    en_ids = [en_ids[i[0]] for i in len_indices] # na potrzeby funkcji pack_padded_sequence\n",
    "    \n",
    "    pl_lens = torch.LongTensor([len(pl) for pl in pl_ids]) # długości sekwencji (przed paddingiem)\n",
    "    en_lens = torch.LongTensor([len(en) for en in en_ids])\n",
    "    pl_maxlen = max(pl_lens)\n",
    "    en_maxlen = max(en_lens)\n",
    "    for pl in pl_ids: # padding danych treningowych do jednej długości\n",
    "        while len(pl) < pl_maxlen:\n",
    "            pl.append(PL_PAD_TOKEN)\n",
    "    for en in en_ids:\n",
    "        while len(en) < en_maxlen:\n",
    "            en.append(EN_PAD_TOKEN)\n",
    "    Y_mask = [[int(x != EN_PAD_TOKEN) for x in y] for y in en_ids] # maska dla paddingu\n",
    "    Y_mask = torch.BoolTensor(Y_mask).transpose(1,0) # wszystkie tensory będą miały kształt (liczba tokenów, liczba sekwencji w batchu)\n",
    "    X = torch.LongTensor(pl_ids).transpose(1,0)\n",
    "    Y = torch.LongTensor(en_ids).transpose(1,0)\n",
    "    return X, Y, pl_lens, en_lens, Y_mask \n",
    "    # zwracamy: wejście do enkodera, wzorcowe wyjście z dekodera, długość sekwencji dla enkodera, i dekodera (obie przed paddingiem), maskę paddingu dla dekodera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQyy949dJfnM"
   },
   "source": [
    "#### Konstrukcja sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25746,
     "status": "ok",
     "timestamp": 1610288084999,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "vqIOzXXw8RFC"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, voc_size, n_layers=1, dropout=0): \n",
    "        # rozmiar wewnętrznej reprezentacji, wielkośc słownika, liczba warstw rekurencyjnych, dropout części rekurencyjnej\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers # część rekurencyjna może mieć kilka warstw\n",
    "        self.hidden_size = hidden_size # rozmiar wewnętrznych reprezentacji\n",
    "        self.embedding = nn.Embedding(voc_size, hidden_size) # warstwa zanurzająca, uczy się kompresować reprezentację one-hot do wektora o długości hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, # gru - mniejszy kuzyn LSTM,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True) # wykorzystujemy wariant dwukierunkowy\n",
    "\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq) # konwersja indeksów słów w wektory\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)# pakowanie batcha danych dla warstwy GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs) # odpakowywanie wyjścia z warstwy GRU\n",
    "        left_to_right = outputs[:, :, :self.hidden_size]\n",
    "        right_to_left = outputs[:, : ,self.hidden_size:]\n",
    "        outputs =  left_to_right + right_to_left  # sumujemy reprezentacje z obu kierunków warstwy rekurencyjnej\n",
    "        return outputs, hidden # zwracamy wyjście i stan ukryty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25744,
     "status": "ok",
     "timestamp": 1610288084999,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "C-3L6zM6z9L_"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module): # warstwa uwagi oparta na artykule Luonga et al.\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method # wybieramy metodę liczenia uwagi\n",
    "\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size)) # warstwa bez biasu\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # dot operuje na zasadzie iloczynu skalarnego stanu ukrytego enkodera i stanu ukrytego dekodera\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        # general wykorzystuje iloczyn skalarny stanu ukrytego dekodera i outputu z warstwy neuronów która przyjmuje stan ukryty enkodera\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        # concat wykorzystuje warstwę neuronów która przyjmuje konkatenację stanu ukrytego enkodera\n",
    "        #   i stanu ukrytego dekodera. Po przejściu przez warstwę aplikowana jest funkcja aktywacji\n",
    "        #   tanh, następnie output przechodzi przez kolejną warstwę neuronów, bez biasu (nn.Parameter)\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # obliczanie wag uwagi na podstawie stanu ukrytego dekodera, i stanów ukrytych enkodera\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "\n",
    "        attn_energies = attn_energies.t() # transpozycja\n",
    "\n",
    "        # zwracamy wagi normalizowane przez softmax\n",
    "        return self.softmax(attn_energies).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25743,
     "status": "ok",
     "timestamp": 1610288085000,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "Mf0pJP879Pv8"
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, voc_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        self.attn_model = attn_model # jedna z trzech metod liczenia uwagi\n",
    "        self.hidden_size = hidden_size # rozmiar reprezentacji wewnętrznych\n",
    "        self.output_size = voc_size # wyjście - reprezentacje one-hot słów angielskich\n",
    "        self.n_layers = n_layers # dekoder może być wielowarstwowy\n",
    "        self.dropout = dropout # dropout dla unikania przeuczania\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(voc_size, hidden_size) # warstwa zanurzająca do reprezentacji inputu (słów zwracanych przez sam dekoder)\n",
    "        self.embedding_dropout = nn.Dropout(dropout) # dropout po warstwie zanurzającej\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size) # warstwa przetwarzająca\n",
    "        self.out = nn.Linear(hidden_size, voc_size) # klasyfikator\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # dekoder przechodzi przez cały batch słowo po słowie (i.e. zbiera najpierw wszystkie pierwsze słowa, potem wszystkie drugie itd.)\n",
    "        embedded = self.embedding(input_step) # zanurzenie  słów\n",
    "        embedded = self.embedding_dropout(embedded) # aplikacja dropoutu\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden) # przejście przez warstwę rekurencyjną\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs) # obliczamy wagi dla mechanizmu uwagi\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) \n",
    "        # bmm to batched matrix multiplication, zwyczajnie mnożymy tutaj wagi przez stany ukryte\n",
    "        rnn_output = rnn_output.squeeze(0)# usunięcie zbędnego wymiaru\n",
    "        context = context.squeeze(1) # usunięcie zbędnego wymiaru\n",
    "        concat_input = torch.cat((rnn_output, context), 1) # konkatenacja wyjścia z GRU, i wyniku mechanizmu uwagi\n",
    "        concat_output = torch.tanh(self.concat(concat_input)) # przejście przez warstwę gęstą z funkcją aktywacji tanh\n",
    "        output = self.out(concat_output) # klasyfikacja - przewidywanie następnych słów\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden # zwracamy przewidywane słowo, i stan ukryty dekodera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMtZB2A9Ge1Y"
   },
   "source": [
    "#### Definicja funkcji straty\n",
    "Potrzebujemy zdefiniować funkcje straty, aby móc maskować stratę na tokenach, które służą paddingowi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25741,
     "status": "ok",
     "timestamp": 1610288085000,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "_iqkVJiA_CLB"
   },
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)) # obliczanie entropii krzyżowej \n",
    "    #(- logarytm z prawdopodobieństwa zwróconego dla wartości docelowej)\n",
    "    loss = crossEntropy.masked_select(mask).mean() # maskowanie straty (względem paddingu)\n",
    "    loss = loss.to(DEVICE)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-EHUpECJZ42"
   },
   "source": [
    "#### Definicja funkcji do treningu i ewaluacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25741,
     "status": "ok",
     "timestamp": 1610288085001,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "9_Jk8lfF_HLA"
   },
   "outputs": [],
   "source": [
    "def train_on_batch(X, X_lens, Y_lens, Y, mask, encoder, decoder, encoder_optimizer, decoder_optimizer, teacher_forcing_ratio, grad_clip):\n",
    "\n",
    "    # Zerowanie gradientów\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    X = X.to(DEVICE)\n",
    "    Y = Y.to(DEVICE)\n",
    "    mask = mask.to(DEVICE)\n",
    "    # Tensory reprezentujące długośc sekwencji (dla pakowania) muszą znajdować się zawsze na CPU\n",
    "    X_lens = X_lens.to(\"cpu\")\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(X, X_lens)# przejście przez enkoder\n",
    "\n",
    "    # Wejście do dekodera w kroku zerowym - tokeny SOS\n",
    "    decoder_input = torch.LongTensor([[EN_SOS_TOKEN for _ in range(BATCH_SIZE)]]).to(DEVICE)\n",
    "\n",
    "    # W kroku zero, stanem ukrytym dekodera, jest stan ukryty z enkodera\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # \n",
    "\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio# losowanie czy w tym przykładzie użyć teacher forcing\n",
    "\n",
    "    max_target_len = max(Y_lens)\n",
    "    if use_teacher_forcing: # teacher forcing\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Za następny input, uznajemy wzorcowy output z tego kroku\n",
    "            decoder_input = Y[t].view(1, -1)\n",
    "            # obliczenie straty\n",
    "            mask_loss = maskNLLLoss(decoder_output, Y[t], mask[t])\n",
    "            loss += mask_loss\n",
    "\n",
    "    else: # sieć radzi sobie sama\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # sieć działa sama, inputem w kroku kolejnym jest output z obecnego kroku\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(BATCH_SIZE)]])\n",
    "            decoder_input = decoder_input.to(DEVICE)\n",
    "            mask_loss = maskNLLLoss(decoder_output, Y[t], mask[t])\n",
    "            loss += mask_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Obcinanie gradientów\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
    "    # Aktualizacja wag\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test_on_batch(X, X_lens, Y_lens, Y, mask, encoder, decoder, teacher_forcing_ratio=0.5):\n",
    "    X = X.to(DEVICE)\n",
    "    Y = Y.to(DEVICE)\n",
    "    mask = mask.to(DEVICE)\n",
    "    X_lens = X_lens.to(\"cpu\")\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(X, X_lens)\n",
    "    decoder_input = torch.LongTensor([[EN_SOS_TOKEN for _ in range(BATCH_SIZE)]])\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    max_target_len = max(Y_lens)\n",
    "    decoder_outputs = []\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_input = Y[t].view(1, -1)\n",
    "            mask_loss = maskNLLLoss(decoder_output, Y[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            decoder_outputs.append(decoder_output.unsqueeze(0))\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(BATCH_SIZE)]])\n",
    "            decoder_input = decoder_input.to(DEVICE)\n",
    "            mask_loss = maskNLLLoss(decoder_output, Y[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            decoder_outputs.append(decoder_output.unsqueeze(0))\n",
    "    decoder_outputs = torch.cat(decoder_outputs)\n",
    "    return loss.item(), decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYHKho4CulJB"
   },
   "source": [
    "#### Przykładowe przejście przez sieć"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Cka4U5vxUos"
   },
   "source": [
    "Stworzenie sieci i przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1444,
     "status": "ok",
     "timestamp": 1610288574891,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "nMm7pftTDwKX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "attn_model = 'general'\n",
    "hidden_size = 600\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.5\n",
    "pl_voc_size = len(pl_voc) + 2\n",
    "en_voc_size = len(en_voc) + 3\n",
    "MAXLEN = 10 + 1\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, pl_voc_size, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, en_voc_size, decoder_n_layers, dropout)\n",
    "attention = decoder.attn\n",
    "\n",
    "examples = data[0:8]\n",
    "batch = examples_to_batch(examples, MAXLEN)\n",
    "X, Y, X_lens, Y_lens, Y_mask = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDe_RVrNxbIk"
   },
   "source": [
    "Przejście przez enkoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1961,
     "status": "ok",
     "timestamp": 1610288580983,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "Rme-FillxOoP",
    "outputId": "2fed72a6-ed8e-443a-b3c3-9d39b6e2cf95"
   },
   "outputs": [],
   "source": [
    "# ENKODER\n",
    "print(\"Kształt wejścia: \", X.shape) # liczba tokenów, numer sekwencji w batchu\n",
    "print(X)# sekwencje są opaddowane i posortowane względem długości\n",
    "embedded = encoder.embedding(X) # konwersja indeksów słów w wektory\n",
    "print(\"Kształt po zanurzeniu: \", embedded.shape) # liczba tokenów, numer sekwencji w batchu, liczba cech\n",
    "packed = nn.utils.rnn.pack_padded_sequence(embedded, X_lens)# pakowanie batcha danych dla warstwy GRU\n",
    "outputs, encoder_hidden = encoder.gru(packed, None) # przekazujemy spakowany input, bez stanu zerowego\n",
    "outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs) # odpakowywanie wyjścia z warstwy GRU\n",
    "print(\"Kształt wyjścia z warstwy rekurencyjnej: \", outputs.shape) # liczba tokenów, numer sekwencji w batchu, liczba cech (z obu kierunków)\n",
    "left_to_right = outputs[:, :, :encoder.hidden_size]\n",
    "right_to_left = outputs[:, : ,encoder.hidden_size:]\n",
    "encoder_outputs =  left_to_right + right_to_left  # sumujemy reprezentacje z obu kierunków warstwy rekurencyjnej\n",
    "print(\"Kształt po agregacji z obu kierunków: \", encoder_outputs.shape) # liczba tokenów, numer sekwencji w batchu, liczba cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6MWF9hvyRVC"
   },
   "source": [
    "Przejście przez dekoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2947,
     "status": "ok",
     "timestamp": 1610288587415,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "tEh7jiBsxaBD",
    "outputId": "ecfb38a7-e590-4ec0-b28e-4d1f497f7408"
   },
   "outputs": [],
   "source": [
    "# DEKODER\n",
    "decoder_input = torch.LongTensor([[EN_SOS_TOKEN for _ in range(BATCH_SIZE)]]) # input w kroku zero - tokeny SOS\n",
    "print(\"Kształt wejścia do dekodera (słowa angielskie): \", decoder_input.shape)# numer tokenu (podajemy po jednym tokenie), numer sekwencji w batchu\n",
    "decoder_hidden = encoder_hidden[:decoder.n_layers] # stan ukryty w kroku zero - stan ukryty z obu warstw enkodera\n",
    "print(\"Kształt stanu ukrytego dekodera: \", decoder_hidden.shape) # numer warstwy, numer w batchu, numer cechy\n",
    "\n",
    "max_target_len = max(Y_lens) # iterujemy aż do końca najdłuższej sekwencji w Y (zdania najdłuższego po angielsku)\n",
    "for t in range(max_target_len): # iterujemy po krokach czasowych\n",
    "    # dekoder przechodzi przez cały batch słowo po słowie (i.e. zbiera najpierw wszystkie pierwsze słowa, potem wszystkie drugie itd.)\n",
    "    embedded = decoder.embedding(decoder_input) # zanurzenie  słów\n",
    "    print(\"Kształt zanurzonych angielskich słów: \", embedded.shape) # numer słowa, numer sekwencji w batchu, numer cechy\n",
    "    embedded = decoder.embedding_dropout(embedded) # aplikacja dropoutu\n",
    "    rnn_output, hidden = decoder.gru(embedded, decoder_hidden) # przejście przez warstwę rekurencyjną\n",
    "    print(\"Kształt wyjścia z warstwy rekurencyjnej: \", rnn_output.shape) # numer słowa, numer sekwencji w batchu, numer cechy\n",
    "    print(\"Kształt stanów ukrytych z warstw rekurencyjnych: \", hidden.shape) # numer warstwy, numer sekwencji w batchu, numer cechy\n",
    "\n",
    "    # ATENCJA\n",
    "    energy = attention.attn(encoder_outputs)\n",
    "    attn_energies = torch.sum(rnn_output * energy, dim=2).t()\n",
    "    print(\"Kształt wag (dla każdego kroku czasowego wejścia):\", attn_energies.shape)\n",
    "    normalized_attn = attention.softmax(attn_energies).unsqueeze(1) # normalizacja funkcją softmax\n",
    "\n",
    "    # DEKODER Ciąg dalszy\n",
    "    context = normalized_attn.bmm(encoder_outputs.transpose(0, 1)) # bmm to batched matrix multiplication, zwyczajnie mnożymy tutaj wagi przez stany ukryte\n",
    "    rnn_output = rnn_output.squeeze(0)# usunięcie zbędnego wymiaru\n",
    "    context = context.squeeze(1) # usunięcie zbędnego wymiaru\n",
    "    print(\"Kształt wektora kontekstu uśrednionego przy pomocy wag: \", context.shape) # numer w batchu, numer cechy\n",
    "    concat_input = torch.cat((rnn_output, context), 1) # konkatenacja wyjścia z GRU, i wyniku mechanizmu uwagi\n",
    "    print(\"Kształt konkatenowanego wejścia do warstwy gęstej (stan ukryty dekodera + wektor kontekstu)\", concat_input.shape) # numer w batchu, liczba cech\n",
    "    concat_output = torch.tanh(decoder.concat(concat_input)) # przejście przez warstwę gęstą z funkcją aktywacji tanh\n",
    "    print(\"Kszałt wejścia do klasyfikatora: \", concat_output.shape) # numer sekwencji w batchu, numer cechy\n",
    "    output = decoder.out(concat_output) # klasyfikacja - przewidywanie następnych słów\n",
    "    decoder_output = decoder.softmax(output)\n",
    "    print(\"Kształt wyjścia z dekodera: \", decoder_output.shape) # numer w sekwencji, numer w słowniku\n",
    "\n",
    "    _, topi = decoder_output.topk(1) # przewidziane słowo\n",
    "    print(\"Przewidziane pierwsze słowa dla każdego ze zdań: \", [en_voc[i] for i in topi])\n",
    "    decoder_input = torch.LongTensor([[topi[i][0] for i in range(BATCH_SIZE)]])\n",
    "    print(\"Wyjście kierowane z powrotem do dekodera: \", decoder_input)\n",
    "    # przesunięcie przewidzianego słowa jako input dla nastepnego kroku\n",
    "    break # zobaczymy tylko jedno przejście\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5qIhD5dxPUI"
   },
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455,
     "referenced_widgets": [
      "dda0cca06a944f1ca95db81c1b34858f",
      "f1d69d82b9914c7a8630ba6c023d8df4",
      "7af36740e7e341fb85d2f9defce24b2f",
      "9de194a07bb947af8f6ec0f7378cd173",
      "6000bb127d024a14a94d80221f4ca9a5",
      "a907c50e79d54a22b76abad7fcba3607",
      "126f365ac3a1428186c1fe2e91db5a09",
      "3e5ddcc913474c2c9039d22d0659da53",
      "00a53c9831df4723a510060643ec3386",
      "cd516902440c46a69ff714d9b735f303",
      "c6a15c9a960d4180947a83f9e76906b7",
      "749848a213d548f6bbf1102a17c8bf4f",
      "d9b77a9325e94359ad443fdbc3c886c2",
      "4af8c4bbe99f4df18f9e39e9382835a1",
      "11560a96765f4f478095f16ea061f4cf",
      "acd56df1c6f7416fb9e0ae45e1a2564b"
     ]
    },
    "executionInfo": {
     "elapsed": 5866970,
     "status": "ok",
     "timestamp": 1610294459252,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "vRZkpbQ1P06R",
    "outputId": "a286ed63-1674-493a-dbdb-5d0bb4b292a5"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "learning_rate=0.0002\n",
    "GRAD_CLIP = 1.0\n",
    "decoder_learning_ratio = 5.0\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "attn_model = 'general'\n",
    "hidden_size = 600\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.5\n",
    "pl_voc_size = len(pl_voc) + 2\n",
    "en_voc_size = len(en_voc) + 3\n",
    "MAXLEN = 10 + 1\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 2\n",
    "encoder = EncoderRNN(hidden_size, pl_voc_size, encoder_n_layers, dropout).to(DEVICE)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, en_voc_size, decoder_n_layers, dropout).to(DEVICE)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "\n",
    "train_data = data[:-5000]\n",
    "dev_data = data[-5000:]\n",
    "TRAIN_ITERS = len(train_data)//BATCH_SIZE\n",
    "DEV_ITERS = len(dev_data)//BATCH_SIZE\n",
    "LOSS_PRINT_INTERVAL = 2000\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    random.shuffle(train_data)\n",
    "    print(\"Epoch no: \", epoch + 1)\n",
    "    interval_losses = []\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for iter in tqdm(range(TRAIN_ITERS)):\n",
    "        examples = train_data[iter*BATCH_SIZE:(iter+1)*BATCH_SIZE]\n",
    "        batch = examples_to_batch(examples, MAXLEN)\n",
    "\n",
    "        # Extract fields from batch\n",
    "        X, Y, X_lens, Y_lens, Y_mask = batch\n",
    "\n",
    "        loss = train_on_batch(X, X_lens, Y_lens, Y, Y_mask, encoder, decoder, encoder_optimizer, decoder_optimizer, TEACHER_FORCING_RATIO, GRAD_CLIP)\n",
    "        interval_losses.append(loss)\n",
    "        if iter % LOSS_PRINT_INTERVAL == 0:\n",
    "            interval_loss = sum(interval_losses)/len(interval_losses)\n",
    "            print(\"{} Interval loss: \".format(iter), interval_loss)\n",
    "            interval_losses = []\n",
    "    with torch.no_grad():\n",
    "        dev_epoch_losses = []\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        for iter in range(DEV_ITERS):\n",
    "            examples = dev_data[iter*BATCH_SIZE:(iter+1)*BATCH_SIZE]\n",
    "            batch = examples_to_batch(examples, MAXLEN)\n",
    "            X, Y, X_lens, Y_lens, Y_mask = batch\n",
    "            dev_loss, _ = test_on_batch(X, X_lens, Y_lens, Y, Y_mask, encoder, decoder)\n",
    "            dev_epoch_losses.append(dev_loss)\n",
    "\n",
    "    dev_epoch_loss = sum(dev_epoch_losses)/len(dev_epoch_losses)\n",
    "    print(\"Dev loss: \", dev_epoch_loss)\n",
    "    dev_losses.append(dev_epoch_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_u7jBWZFjri"
   },
   "source": [
    "#### Zapisywanie i wczytywanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2144,
     "status": "ok",
     "timestamp": 1610296949382,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "bQtGqBTiK-d0"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder, \"encoder.model\")\n",
    "torch.save(decoder, \"decoder.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ohg4iPmIcRH7"
   },
   "source": [
    "Tę komórkę należy wywołać, aby ominąć trening, i wczytać gotowy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1736,
     "status": "ok",
     "timestamp": 1610296952637,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "npHMf9IfaWHh"
   },
   "outputs": [],
   "source": [
    "encoder = torch.load(\"encoder.model\")\n",
    "decoder = torch.load(\"decoder.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJbkq5A9Fh-l"
   },
   "source": [
    "#### Testowanie na zdaniach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1813,
     "status": "ok",
     "timestamp": 1610296955008,
     "user": {
      "displayName": "Martin Heidegger",
      "photoUrl": "",
      "userId": "01507638503378006146"
     },
     "user_tz": -60
    },
    "id": "gtj4Fem3Kv58",
    "outputId": "2ae181da-24e7-401c-ac9a-9f9303bf0055"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "MAXLEN = 10 + 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "def test_on_sent(sentence):\n",
    "    tokenizer = re.compile(r\"[\\w]+\")\n",
    "    tokens = [t for t in tokenizer.findall(sentence.lower()) if t in pl_voc]\n",
    "    pl_ids = [[pl_voc.index(t) for t in tokens]]\n",
    "    X_lens = torch.LongTensor([len(pl) for pl in pl_ids])\n",
    "    pl_maxlen = max(X_lens)\n",
    "    X = torch.LongTensor(pl_ids).transpose(1,0).to(DEVICE)\n",
    "    encoder_outputs, encoder_hidden = encoder(X, X_lens)\n",
    "    decoder_input = torch.LongTensor([[EN_SOS_TOKEN for _ in range(BATCH_SIZE)]]).to(DEVICE)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    max_target_len = MAXLEN\n",
    "    decoder_outputs = []\n",
    "    top_outs = []\n",
    "    for t in range(max_target_len):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        top_outs.append(topi.item())\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(BATCH_SIZE)]]).to(DEVICE)\n",
    "        decoder_outputs.append(decoder_output.unsqueeze(0))\n",
    "    decoder_outputs = torch.cat(decoder_outputs) \n",
    "    en_words = [en_voc[i] for i in top_outs if i!=EN_EOS_TOKEN]\n",
    "    return en_words\n",
    "\n",
    "translation = test_on_sent(\"Lubię czytać długie książki\")\n",
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "translator.ipynb",
   "provenance": [
    {
     "file_id": "1BfMYeoT04EQvWdLVLM_XpLp6zbkmQ0kF",
     "timestamp": 1609191725326
    },
    {
     "file_id": "15sBXtaOqi8677kzukrUVi1ftYTNWeo4o",
     "timestamp": 1609067421124
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00a53c9831df4723a510060643ec3386": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6a15c9a960d4180947a83f9e76906b7",
       "IPY_MODEL_749848a213d548f6bbf1102a17c8bf4f"
      ],
      "layout": "IPY_MODEL_cd516902440c46a69ff714d9b735f303"
     }
    },
    "11560a96765f4f478095f16ea061f4cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "126f365ac3a1428186c1fe2e91db5a09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e5ddcc913474c2c9039d22d0659da53": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4af8c4bbe99f4df18f9e39e9382835a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6000bb127d024a14a94d80221f4ca9a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "749848a213d548f6bbf1102a17c8bf4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acd56df1c6f7416fb9e0ae45e1a2564b",
      "placeholder": "​",
      "style": "IPY_MODEL_11560a96765f4f478095f16ea061f4cf",
      "value": " 15546/15546 [48:49&lt;00:00,  5.31it/s]"
     }
    },
    "7af36740e7e341fb85d2f9defce24b2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a907c50e79d54a22b76abad7fcba3607",
      "max": 15546,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6000bb127d024a14a94d80221f4ca9a5",
      "value": 15546
     }
    },
    "9de194a07bb947af8f6ec0f7378cd173": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e5ddcc913474c2c9039d22d0659da53",
      "placeholder": "​",
      "style": "IPY_MODEL_126f365ac3a1428186c1fe2e91db5a09",
      "value": " 15546/15546 [48:41&lt;00:00,  5.32it/s]"
     }
    },
    "a907c50e79d54a22b76abad7fcba3607": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acd56df1c6f7416fb9e0ae45e1a2564b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6a15c9a960d4180947a83f9e76906b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4af8c4bbe99f4df18f9e39e9382835a1",
      "max": 15546,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9b77a9325e94359ad443fdbc3c886c2",
      "value": 15546
     }
    },
    "cd516902440c46a69ff714d9b735f303": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9b77a9325e94359ad443fdbc3c886c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dda0cca06a944f1ca95db81c1b34858f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7af36740e7e341fb85d2f9defce24b2f",
       "IPY_MODEL_9de194a07bb947af8f6ec0f7378cd173"
      ],
      "layout": "IPY_MODEL_f1d69d82b9914c7a8630ba6c023d8df4"
     }
    },
    "f1d69d82b9914c7a8630ba6c023d8df4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
